{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bb745958eaa5407099fb6308d65a27c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5d056804fe14062b0dff8a56219d822",
              "IPY_MODEL_876f89625c2643c89b1e37800cc2a4ec",
              "IPY_MODEL_8a1d016e57a24e04a9eb31b77dceb045"
            ],
            "layout": "IPY_MODEL_c5295fb0de4640abb3f90081286533cf"
          }
        },
        "f5d056804fe14062b0dff8a56219d822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a68b2792760944ed9e83bcb194d0d77e",
            "placeholder": "​",
            "style": "IPY_MODEL_b390bcd6b3dc473ebfd23f4b25da5689",
            "value": "Map: 100%"
          }
        },
        "876f89625c2643c89b1e37800cc2a4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50e567a4f46b46ec9d48416f7e58664e",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4aa25933b824295ba960aafb0bb2122",
            "value": 50000
          }
        },
        "8a1d016e57a24e04a9eb31b77dceb045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad773510ea1b42e980dbe797398f7ec2",
            "placeholder": "​",
            "style": "IPY_MODEL_541cea2bcaca442380610df33e5aff13",
            "value": " 50000/50000 [00:12&lt;00:00, 6290.77 examples/s]"
          }
        },
        "c5295fb0de4640abb3f90081286533cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a68b2792760944ed9e83bcb194d0d77e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b390bcd6b3dc473ebfd23f4b25da5689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50e567a4f46b46ec9d48416f7e58664e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4aa25933b824295ba960aafb0bb2122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad773510ea1b42e980dbe797398f7ec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "541cea2bcaca442380610df33e5aff13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_iRAUor1XDp3",
        "outputId": "859fb1b6-9263-4a69-af36-fb1a81c41de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Collecting peft\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, bitsandbytes, peft, datasets\n",
            "Successfully installed bitsandbytes-0.44.1 datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 peft-0.13.2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install transformers accelerate peft datasets bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import (\n",
        "    MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "ZVvOsiOLXIon"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"hf://datasets/salehalmansour/english-to-arabic-translate/en_ar_final.tsv\", sep=\"\\t\")\n",
        "df = df[['en', 'ar']]\n",
        "df = df.head(50000)\n",
        "dataset = Dataset.from_pandas(df)\n"
      ],
      "metadata": {
        "id": "3LS_7-HZXvIm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Helsinki-NLP/opus-mt-en-ar\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iSZM5Q38YDDe",
        "outputId": "7e7cf22d-9295-438e-8f7e-67526971c68e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=2,           # Rank of the low-rank update matrices\n",
        "    lora_alpha=16,  # Scaling factor for LoRA updates\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # LoRA applied to attention layers\n",
        "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
        "    bias=\"none\"     # No bias added in LoRA layers\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n"
      ],
      "metadata": {
        "id": "8rOolQFlYR0V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(examples['en'], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    targets = tokenizer(examples['ar'], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    inputs['labels'] = targets['input_ids']\n",
        "    return inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "bb745958eaa5407099fb6308d65a27c9",
            "f5d056804fe14062b0dff8a56219d822",
            "876f89625c2643c89b1e37800cc2a4ec",
            "8a1d016e57a24e04a9eb31b77dceb045",
            "c5295fb0de4640abb3f90081286533cf",
            "a68b2792760944ed9e83bcb194d0d77e",
            "b390bcd6b3dc473ebfd23f4b25da5689",
            "50e567a4f46b46ec9d48416f7e58664e",
            "c4aa25933b824295ba960aafb0bb2122",
            "ad773510ea1b42e980dbe797398f7ec2",
            "541cea2bcaca442380610df33e5aff13"
          ]
        },
        "id": "-Z1EjpuxZC_W",
        "outputId": "3de5b54a-c888-425c-a771-261c9167e8e5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb745958eaa5407099fb6308d65a27c9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./lora-opus-mt-en-ar\",  # Save directory\n",
        "    evaluation_strategy=\"steps\",        # Evaluate every few steps instead of epochs\n",
        "    eval_steps=500,                     # Evaluate every 500 steps (adjust based on dataset size)\n",
        "    learning_rate=3e-4,                 # Increase learning rate for faster convergence\n",
        "    per_device_train_batch_size=16,     # Increase batch size if GPU memory allows\n",
        "    per_device_eval_batch_size=16,      # Match the evaluation batch size\n",
        "    gradient_accumulation_steps=2,      # Accumulate gradients to simulate larger batch size\n",
        "    weight_decay=0.01,                  # Keep weight decay for regularization\n",
        "    save_total_limit=1,                 # Reduce saved checkpoints to save disk space\n",
        "    save_steps=1000,                    # Save checkpoints every 1000 steps\n",
        "    num_train_epochs=1,                 # Use fewer epochs to reduce total time\n",
        "    predict_with_generate=True,         # Still generate predictions for validation\n",
        "    logging_dir=\"./logs\",               # Directory for logs\n",
        "    logging_steps=100,                  # Log every 100 steps to reduce logging overhead\n",
        "    report_to=\"none\",                   # Disable logging to external tools (e.g., TensorBoard)\n",
        "    fp16=True,                          # Use mixed precision for faster training\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opNtan9VZxaq",
        "outputId": "397b83dd-fe45-49fd-dc52-159617b44bd4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,  # Using same dataset for simplicity (can be split)\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "SwArMnDKcI6D",
        "outputId": "ed185d71-aa9c-422b-a7c5-57c824f1f85e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1562' max='1562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1562/1562 15:28, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.260700</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.247600</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.235700</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1562, training_loss=0.3704145684284987, metrics={'train_runtime': 930.1899, 'train_samples_per_second': 53.752, 'train_steps_per_second': 1.679, 'total_flos': 1697206121791488.0, 'train_loss': 0.3704145684284987, 'epoch': 0.99968})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./lora-opus-mt-en-ar\")\n",
        "tokenizer.save_pretrained(\"./lora-opus-mt-en-ar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3uNn-sncMoa",
        "outputId": "33045555-3948-4ce1-a835-4dfa441ba91c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./lora-opus-mt-en-ar/tokenizer_config.json',\n",
              " './lora-opus-mt-en-ar/special_tokens_map.json',\n",
              " './lora-opus-mt-en-ar/vocab.json',\n",
              " './lora-opus-mt-en-ar/source.spm',\n",
              " './lora-opus-mt-en-ar/target.spm',\n",
              " './lora-opus-mt-en-ar/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r lora-opus-mt-en-ar.zip ./lora-opus-mt-en-ar\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"lora-opus-mt-en-ar.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "SNP7jbAqjFso",
        "outputId": "9fdaa39b-9a41-48ba-9853-377d86e3899d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: lora-opus-mt-en-ar/ (stored 0%)\n",
            "  adding: lora-opus-mt-en-ar/checkpoint-1562/ (stored 0%)\n",
            "  adding: lora-opus-mt-en-ar/checkpoint-1562/scheduler.pt (deflated 56%)\n",
            "  adding: lora-opus-mt-en-ar/checkpoint-1562/training_args.bin (deflated 51%)\n",
            "  adding: lora-opus-mt-en-ar/checkpoint-1562/optimizer.pt (deflated 12%)\n",
            "  adding: lora-opus-mt-en-ar/checkpoint-1562/adapter_config.json (deflated 53%)\n",
            "  adding: lora-opus-mt-en-ar/checkpoint-1562/rng_state.pth (deflated 25%)\n",
            "  adding: lora-opus-mt-en-ar/checkpoint-1562/README.md (deflated 66%)\n",
            "  adding: lora-opus-mt-en-ar/checkpoint-1562/trainer_state.json (deflated 73%)\n",
            "  adding: lora-opus-mt-en-ar/checkpoint-1562/adapter_model.safetensors (deflated 9%)\n",
            "  adding: lora-opus-mt-en-ar/adapter_config.json (deflated 53%)\n",
            "  adding: lora-opus-mt-en-ar/source.spm (deflated 49%)\n",
            "  adding: lora-opus-mt-en-ar/README.md (deflated 66%)\n",
            "  adding: lora-opus-mt-en-ar/tokenizer_config.json (deflated 68%)\n",
            "  adding: lora-opus-mt-en-ar/target.spm (deflated 55%)\n",
            "  adding: lora-opus-mt-en-ar/vocab.json (deflated 77%)\n",
            "  adding: lora-opus-mt-en-ar/adapter_model.safetensors (deflated 9%)\n",
            "  adding: lora-opus-mt-en-ar/special_tokens_map.json (deflated 35%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ebf7c2d2-69b3-46d9-a257-40860faa099b\", \"lora-opus-mt-en-ar.zip\", 2479123)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"./lora-opus-mt-en-ar\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name_or_path)\n",
        "model = MarianMTModel.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhmUR8svkHOw",
        "outputId": "bb2db081-d79c-4d82-f1b5-2675ec9e9795"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Pickle Data\n",
        "\n",
        "import pickle\n",
        "with open('/content/Transcribed_Sentences (1).pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "english_sentences = [item['text'] for item in data]\n",
        "\n",
        "print(english_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe1i3qGskRQc",
        "outputId": "21eb8c3a-6369-4908-d070-5b00cdcd3436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' She hides her grief and joins in the homecoming celebration.', ' Rokshik stayed at the palace of his personal friend, the dictator Fernandez Marcos.', ' She had an uneventful career.', \" Let's eat the chocolate tonight.\", ' Newport was a county borough.', ' The Heckman model falls into this type.', ' The writing lost grain fleet in the rural southwestern corner of St. Catharines.', ' It features a day and night camera and video relay.', ' More modern versions of the tax no longer require an actual stamp.', ' The classification is based on the timing of transcription that is temporarily regulated.', ' He wore a white sitter to dinner.', ' It forms the largest artificial lake in Maui, Lake Manantali.', \" Fox shuffled the show's airtime repeatedly to make room for other shows.\", ' Viner has produced short documentary videos about legal history and comparative law.', ' When the player character touches a monster, they die.', ' The Customs and Border Protection offices recently opened in the easternmost former cargo building.', ' Classmates and friends endowed a scholarship in his name at Carnegie Mellon.', ' This originated in slavery times where slaves were not allowed to play musical instruments.', ' In autumn that year, she made a solo tour with her band in Switzerland.', ' The building houses, classrooms, toilets, showers, and a staff room.', ' She was nicknamed Long Liz.', ' The winners of each pairing, higher than bold, qualify for the following round.', ' Many families have moved into these houses.', ' Fitchell seems likely to have been played with equal numbers on both sides.', ' Air Academy has received national acclaim for its numerous instrumental ensembles.', ' Call the Doctor was written in three weeks and recorded in four days.', ' He retired from basketball after the season.', ' For more details on geological crystal formation, see above.', ' This document shows that the Catholic Church is officially moved to unity.', ' The marriage resulted in a rift between mother and son.', ' Billshare often structures its investment mandates to meet specific objectives.', ' Its supports had to be attached to the Mensa.', ' Fuel was delivered via multiport fuel injection.', ' No tree may be felled on the holy ground.', ' Despite the movement of the Ferd Mines, fighting continued throughout the country.', ' Mr. Mallory was hit by a car on his way to work.', ' It is not thus used in England.', ' Mach-Eldalon production was a result of correcting a dislike of symmetry.', \" Carlson High School's mascot is the senator.\", ' Transport network analysis falls within the field of transport engineering.', ' I will review some of the notable developments in the economy and financial system.', ' Trent manages to secure a small fighter and departs for Severa.', ' People always back up in small country lanes to let you pass.', ' The ancients were also called sorers for their common appearance in the air.', ' This will lead to a more active participation from the patient side.', ' Types with two or three sets of guide cathodes could count in either direction.', ' In Britain, the earliest records of this surname appear mainly in the West Country.', ' This remarkable discovery was unprecedented at the time.', ' Telescopes with apertures of or larger are needed to distinguish structures in the galaxy.', ' It was named Christmas Fairyland.', ' The single storage shed contains blocked arches.', ' It is hosted by Anne-Marie Mediwake and Ben Moroni.', ' Caledonian decided to apply for membership, as did city rivals Inverness Thistle.', ' The longer lines have four accents, and the shorter ones usually only three.', ' This higher level is associated with strong individual approaches to problem solving and creativity.', \" The ending also implies that her strength exceeded Karen's.\", ' The village is open each weekend from May to the end of September.', ' Videographers use nonlinear editing software on home computers.', ' This is a close-up view of the main character currently being played.', ' This institution is a prestigious university in Brazil.', ' The Matilda Tower at Point Clay has been converted into a war museum.', ' Shortly after he signed, the Alouettes folded and the franchise began the Montreal Concords.', ' Vautour is the French word for vulture.', ' He published his earliest works for the cello.', ' First, there is not one standard transcription in use even for standard Arabic.', ' Redmond helicopter arrives on the scene with a film for everyone to watch.', ' However, not merely is there no record of the wedding.', ' The Center does not award graduate degrees.', \" Traffic is really bad today, probably because it's a free day weekend.\", ' Diagnosis is made by Biomicroscopic Examination in the Clinic.', ' Trunking also refers to the connection of switches and circuits within a telephone exchange.', ' Like most of the Batman family, Stephanie Brown has no superhuman powers.', ' Andrew leads a revolt and takes a council member hostage.', ' He suddenly goes into convulsions himself and soon transforms into an alien creature himself.', ' Liberia then liquidated the property of German nationals in Liberia.', ' Despite hard work and rough seas, the ships only found one mine.', ' Því lægileg hér með síx manns íntu fís ríinn.', ' The local council has made concurrent improvement to the surrounding area.', ' Our argument may be briefly indicated in advance as follows.', \" Guinea's World Records considers it a production motorcycle.\", ' Harmonic functions are the classical example to which the strong maximum principle applies.', ' He also wrote an empire of dust about the dust bowl.', ' However, month by month breakdowns for domestic sales of individual car models are available.', \" However, it isn't at all about itself geographically.\", ' It has now ended its third season.', ' The executive wing is headed by the Commissioner.', ' Then one gives rigorous mathematical constructions of examples satisfying these axioms.', ' The Devonport Leet, a man-made water channel, passes nearby.', ' The Ithaca Journal produces a youth-oriented tabloid-sized publication called Buzz every week.', ' It is one of the thinnest steel and concrete towers in the world.', ' As a teenager in St. Louis, Missouri, he played with Charlie Creith.', ' Graduates of the school are prepared to pursue an artistic and theatrical occupation.', ' In contrast, the backwash is dominated by the sheet flow and bed load sediment transport.', ' Silo is served by Silo High School.', ' The drug is administered for 7 to 10 days.', ' The gardens are open to the public daily.', ' Defines the genre.', ' Therefore, the defendants prevailed and did not have to pay.', ' A local woodcutter discovered a stream that ran with fine sake.', ' Katronis was named by Homer.', ' Women competed in the steeplechase for the first time.', ' Poyakov underwent many car accidents before, during and after the flight.', ' A terrorist group called Kryon Lions take control of the space-based laser weapon.', ' They got engaged at Comate George Lodge in Muntumalanga.', ' Vísen sjóðjara karlín var í um ístu fyrir þér.', ' Until his death, Bettelheim lived in Paris.', ' Rain and wind also cause temporal variation to the phyllosphere microbiome.', ' Castles went on to investigate the Lhasa outbreaks in West Africa.', ' He took part in the American War of Independence.', ' They had spent them previous three years in a refugee camp in Sudan.', ' She continues to perform regularly at live venues and appears on television and radio.', ' He was on the post-season roster where he made one appearance without an at-bat.', ' These interactions are joined with other roads.', ' His older brother was film school composer James Horner.', ' The third test was washed out after only three hours play.', ' Six to eight pairs of nerves are formed.', ' Consequently, there are also two types of oats.', ' They also vary in the geometry of the glycosidic linkage.', \" The group was renamed Justice for Kashmir, then renamed People's Justice Party.\", ' They are known to be loud and loud louder than standard hub motor fans.', ' Shops of various offerings and items are also set up for people to buy.', ' At the time, May and Ernest Eilind were living apart.', ' The district government originally opposed the construction of the museum.', ' His son was taken by one of his ministers and hidden.', ' Prior to incorporation, the town was known as Blaisdell Town.', ' The Kata Center also collaborates with other public and private organizations', ' This is a very simple model of how whole conduction works.', ' The process of induction relies entirely on members proposing new members.', ' Macroecology approaches the idea of studying ecosystems using a top-down approach.', ' She wraps the baby in a shawl and leaves the house.', ' In addition, many can, in theory, support an unlimited number of species.', \" Watch on your joke, iPhone news, I can't.\", ' To the east of the hills found an Iron Age earthwork.', ' A nighttime version of concentration took over its time slot the following week.', ' Sen remained with the club for another season.', ' In each case, the adaptation was scripted by Binder and drawn by Joe Orlando.', ' The dog jumped for joy at the sight of her owner.', ' His younger brother Arl trained as an ophthalmologist.', ' The upper house is the senate.', ' The Japanese bombing raids were also fiercely contested, sometimes with significant Japanese losses.', ' As the family argues, Lucy slips out unnoticed, unsure of her future.', ' It is brown and green algae, the water plant, and the substrate is overgrown.', ' Bacteria probably have the largest capacity to utilize amino acids.', ' She has also played for the Los Angeles Temptation in the Legends Football League.', ' He achieved the rank of Colonel and died in London.', ' But the bride is nowhere to be found.', ' These do not require a marriage license.', ' He was especially interested in Middle Eastern history and the Jewish remarks.', ' Likewise, others also feature one stone.', ' Bye then, strike forward.', ' Within each queue, packets are forwarded based on first-in-first-out basis.', ' Some of the suspects in the shooting are still at large.', ' IAB completed the occupation of Rewin land by capturing Hadar in the Bakul region.', ' It was located in the city of Edmonton in the province of Alberta.', ' He died in Tampa, Florida and was buried in the Roseland Cemetery of Monticello.', ' He later played for Old Han.', ' Not all cyclists were competing for the victory, some only joined as tourists.', ' Then the columns are permitted and then the rows are permitted.', \" My Google's research has also included computer architecture, computer security, and hardware software code design.\", ' The theory of buildings has important obligations in several rather disparate fields.', ' Rabal is reported to donate all proceeds from the book to charity.', ' The judge are not given any brand information.', ' Born in Chicago, Illinois, Thomas grew up in Pittsburgh, Pennsylvania.', ' She divorced to Garson, whom she married in London, to Mary Bear.', ' Large amounts of animal habitat was destroyed.', ' The museum will sit on and has been given a building.', ' The remaining plays of the Tetralogy have been mostly lost.', ' The album also peaked at number 6 in Germany, but was certified gold there.', ' The cartridge is intended for a large and dangerous game.', ' This was achieved due to the steadfast dedication of Doig and Jafari.', ' The game was released under different titles in each region.', ' Byrne attended the Baltimore City College High School and Wake Forest College.', ' The district is located in the middle part of the Black Forest Mountains.', ' He has two daughters, Lexi and Maya.', ' Sharp contrast in the edges of the letters are also featured throughout his artwork.', ' This virtually guaranteed a death sentence.', ' He was educated at an Anglican school and was apprenticed as a cabinet maker.', ' Tatars were also deported from Crimea.', ' He was the father of Norazin and Yoritsun.', ' He is married to Kristen Gore.', ' Victoria won the inaugural event.', ' He colored the box in with diagonal stripes.', ' His brother Justin also played basketball at Boulder Creek High School.', ' We are hosting some new events from now.', ' His dad was heartbroken.', ' Most of these families come from Goose Creek Township.', ' The former City Hall is now the North York Civic Center.', ' Solidarity was also declared a legal organization.', \" McDonald's announced they were looking into possible copyright infringement of the name.\", ' It was originally recorded by LaFace Records artist Sam Salter.', ' Vectored Vaccines is another next generation vaccine.', ' Subsequent withdrawals by Chinese skaters were also replaced by Italian skaters.', ' He has contributed importantly to the history and origin of the modern Malay language.', ' Surrounding villages are copy Thorn to the northeast and Bartley to the southeast.', ' He arrived in the town immediately after its capture by the Royal Serbian army.', ' It is the county seat of Trimble County.', ' From those we then tried to get a contradiction.', ' He may be regarded as one of the founders of petrographical science.', ' Users can then use content filtering software to censor various types of content.', ' This can be shown by assigning two distinct numerical values to the dichotomous variable.', ' It is the headquarters of Thunderland City Council.', ' Its seat is located in the town of Mora.', ' Doc lives in St. Clair, Michigan with his wife, Joyce.', ' The Allen area was previously home to the Caddo Comanche and other indigenous peoples.', ' when quinn mentioned that he was drawn to acting wright encouraged him', ' He later would invite Adam Lazara to join the band.', ' The following is the Serbia roster in the Olympic qualifying tournament.', ' Subscribers received a disk with the scenario files already created.', ' Finus Ockham and Scotus wrote commentaries on On Interpretation.', ' However, Thornley severed his connection with Better Place only three months later.', ' Applicants must have resided in the country for five years.', ' Under the current system, supervisors are elected by district to four-year terms.', ' The same principle is taken to an extreme in I-beams.', ' In Germanic mythology, Eilren is the wife of Agilaz, the legendary archer.', ' He regularly visited Krause for advice.', ' Thus, Maximin in the original position represents a formulation of social equality.', ' He was a property consultant with a real estate firm before entering politics.', \" The latter became the costliest hurricane in Maine's history.\", ' Production has many functions and diversity is the foundation of such production.', ' It is an Eastern Iranian language belonging to the Indo-European family.', ' Some would say progressive rock at its best.', ' This is perhaps a mitigation of an older sentence of death by drowning.', ' The village is served by Gilfok Fargoed Railway Station.', ' McCall has pioneered cancer immunotherapies for children.', ' An outspoken girl who was mistreated by Neil.', ' They found the answer to be in the affirmative.', ' In American football, midfield refers to the halfway line.', ' Saudi Arabia entered one Karateka into the inaugural Olympic tournament.', ' By measuring the diameter of the ring, its total age could be estimated.', ' Kerner was particularly active in the fields of phytogeography and phytosociology.', ' The National Assembly has the power to remove government ministers from their post.', ' The board appealed the decision to the Supreme Court.', ' The city of Medford lies mostly within the town boundaries.', ' Movie Guide rates movies for both quality and acceptability.', ' Whitewash is based on the same chemistry.', ' It merged with Cleveland scene.', ' The associated steam raising plant and hydraulic pumps have been removed.', ' In the jungle, Lyra is affected by the deadly disease and shortly dies.', ' Miriam read also, obliterating herself.', ' She earned a Golden Globe Award nomination for the role.', ' He has continued his career by appearing in various cinema and television productions.', ' In the 19th century, Plainville was served by the Farmington Canal.', ' Fortunately for patients afflicted with parosmia, symptoms usually decrease with time.', ' Another anglization was pick of where.', ' In parliamentary elections in September, the party won four seats.', ' to upgrade the standard of education in Pakistan.', ' Only Shi Kasina and Shi Kriana, who were visiting West Germany, escaped.', ' The unincorporated community of West Almond is located in the town.', ' A positive result would be the jerking of the foot towards its plantar surface.', ' The Slow Media is a media empire in the East, Switzerland.', ' Such rides were the precursor of the modern log ride amusement park attractions.', ' Modern doubled dyes in American coinage are being discovered mainly in Lincoln sense.', ' He also encouraged development of the mining industry and commerce.', ' Users at home of the underdogs gave Treasure Cove.', ' Their hostess, who had initially denied their presence, was charged with harboring traitors.', ' He then studied dentistry in Freiburg and Munich.', ' Here it comes!', ' These weapons were used in the shock and awe phase of the Iraq War.', ' There are also corner shops on the estates.', \" The suicide of Judas is an execution in which Bezelbub performs the hangman's duty.\", ' Stone never married.', ' He toured Fiji and Tonga with England later that year.', ' He was later selected by the Conservatives as a European parliamentary candidate.', ' The Luck of the Roaring Camp is a short story by American author Bret Hart.', ' He is married to the former Nancy Dorn.', ' This event is known in paleontology as the Great American Interchange.', ' Like all test coins, the platinum farthing has a high historic and numismatic value.', ' Danny runs in to help him, but is too late.', ' An abbreviated English translation was also made from the Latin by purchase.', ' This started the coaching revolution.', ' Bergamot contains extremely large amounts of polyphenols as compared to other citrus species.', ' The end of the play was indicated by the closing of the front curtain.', ' The unit served in the Bristow campaign.', ' Many such harbors are rios.', ' In France, he broke the world record and he received a financial bonus.', ' He later assisted to create numerous organizations to preserve the culture of Nepal.', ' Clements Junior High School', ' The tax was unique in several respects.', ' Other names more rarely used include Downy Ash, Swamp Ash, and Water Ash.', ' The blazon of the Municipal Coat of Arms is Argent A. Cauldron Ghouls.', ' They had a fight in front of the class.', ' Soon after, Concepcion followed the mother figure of merengue, Mili Quesada.', \" Boomsong's younger brother, Yannick Boomsong, is also a professional footballer.\", ' The lieutenant governor is also limited to two consecutive four-year terms.', ' The City later issued a statement reversing their interpretation of the sign regulations.', ' In addition, tutorials, state-of-the-art surveys, and hot topics are increasingly being included.', ' It is designed for civilian use.', ' Support for his cause quickly grew.', ' She was also the first Latina to graduate from Loyola Law School.', ' The commercial species of pistachio has larger fruits and is edible.', ' This arrangement did not last very long.', ' Most of his albums are mounted in accordion fashion.', ' Crews expected to complete one lift each week.', ' Students began attending classes in the new building when they returned from spring break.', ' Guest political analysts include educators, lobbyists, and former legislators.', ' After the war, some of the statues were brought back to Hong Kong.', ' Shiloh was once a stop on the old Winchester and Western Railroad.', ' After that time, he represented the riding of Vancouver Quadra until his defeat.', ' Gilbert has one son and two grandchildren.', ' He was interred in Somerville Cemetery in Augusta, Georgia.', ' Kudrow was raised in a middle-class Jewish family and had a bat mitzvah ceremony.', ' Kenyon was an officer in the British Indian Army.', ' The prize has been called the Nobel Prize of Music in Sweden.', \" We've been stuck in the queue by those roadworks for ages.\", ' Then the actual Kringle Throw starts.', ' The pool is an outdoor pool and is managed by the Caveside Recreation Committee.', ' Samuel English was born in the hamlet of Crebolia in Agaddawe, Northern Ireland.', ' 24 of her crew were killed or missing and 9 wounded.', ' It targets a young readership and is owned and managed by Diligent Media Corporation.', ' Weeks is an American political scientist.', ' Love rearranged parts of the song melodically and used entirely different lyrics.', \" April goes door to door searching for Warren's residence.\", ' It uses radio waves to ionize and heat a propellant.', ' This is the last season to be aired in standard definition.', ' Channel Geldam, I came via train.', \" Projectro's attitude has become increasingly cold, detached, and almost selfish in nature.\", ' When Galan finds out, it is already too late, and the damage is irrevocable.', ' Coons is married and has three sons.', ' The town is home to Genesee Country Village and Museum.', ' Yes, it was Downey.', ' Q values are also featured in particle physics.', ' Representatives from the Treasury may attend the meeting, but only as non-voting observers.', ' You might be interested in that.', ' Contemporary reviewers often alluded to these sounds.', ' She had only a very small fleet and did not provide long-haul routes.', ' Episodes of the series were also released in a Japanese LaserDisc set.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Translating\n",
        "\n",
        "arabic_sentences = english_sentences.copy()\n",
        "index = 0\n",
        "for sentence in english_sentences:\n",
        "  print(index)\n",
        "  input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "  output_ids = model.generate(input_ids)\n",
        "  translated_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "  arabic_sentences[index] = translated_sentence\n",
        "  index += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "15ep3Wj7kleo",
        "outputId": "7d44c5be-777f-4aa7-9aa1-fe66104ce655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Data\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'English': english_sentences, 'Arabic': arabic_sentences})\n",
        "df.to_csv('Translated_data.csv', index=False)"
      ],
      "metadata": {
        "id": "9PAHivLDkp65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.iloc[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NyQcIC9rQde",
        "outputId": "d1822312-2880-4747-ecd7-aa37c0936039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English     Let's eat the chocolate tonight.\n",
            "Arabic                        لنأكل الشكاولة\n",
            "Name: 3, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # How to call the fine-tuned model\n",
        "# # Load the original model\n",
        "# base_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
        "\n",
        "# # Load the LoRA adapters\n",
        "# model = PeftModel.from_pretrained(base_model, \"./lora-opus-mt-en-ar\")\n",
        "\n",
        "# # Now the model has both the original weights and the fine-tuned LoRA parameters.\n"
      ],
      "metadata": {
        "id": "x2uiriwApejY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from datasets import Dataset\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Ensure you have the necessary NLTK packages\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"hf://datasets/salehalmansour/english-to-arabic-translate/en_ar_final.tsv\", sep=\"\\t\")\n",
        "df = df[['en', 'ar']].head(300)  # Get the first 300 lines\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_name_or_path = \"./lora-opus-mt-en-ar\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name_or_path)\n",
        "model = MarianMTModel.from_pretrained(model_name_or_path)\n",
        "\n",
        "# Function to translate sentences\n",
        "def translate(sentences):\n",
        "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    outputs = model.generate(**inputs)\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Translate the English sentences\n",
        "english_sentences = dataset['en']\n",
        "translated_arabic_sentences = translate(english_sentences)\n",
        "\n",
        "# Your BLEU and ROUGE functions\n",
        "def CalculateAvgBLEUScore(Transcribed_Sentences, Validated_Sentences, tokenizer):\n",
        "    BLEU_Scores = []\n",
        "    for i in range(len(Transcribed_Sentences)):\n",
        "        Valid_Sentence = tokenizer(Validated_Sentences[i])  # Tokenize validated sentence\n",
        "        Transcribed_Sentence = tokenizer(Transcribed_Sentences[i])  # Tokenize transcribed sentence\n",
        "        BLEU_Scores.append(nltk.translate.bleu_score.sentence_bleu([Valid_Sentence], Transcribed_Sentence))\n",
        "    return np.mean(BLEU_Scores)\n",
        "\n",
        "def CalculateAvgROUGEScore(Transcribed_Sentences, Validated_Sentences):\n",
        "    rs = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    ROUGE1_Percision_Scores = []\n",
        "    ROUGE1_Recall_Scores = []\n",
        "    ROUGE1_F1_Scores = []\n",
        "    ROUGE2_Percision_Scores = []\n",
        "    ROUGE2_Recall_Scores = []\n",
        "    ROUGE2_F1_Scores = []\n",
        "    ROUGEL_Percision_Scores = []\n",
        "    ROUGEL_Recall_Scores = []\n",
        "    ROUGEL_F1_Scores = []\n",
        "\n",
        "    for i in range(len(Transcribed_Sentences)):\n",
        "        rouge_score = rs.score(Validated_Sentences[i], Transcribed_Sentences[i])  # Directly score the strings\n",
        "        ROUGE1_Percision_Scores.append(rouge_score['rouge1'][0])\n",
        "        ROUGE1_Recall_Scores.append(rouge_score['rouge1'][1])\n",
        "        ROUGE1_F1_Scores.append(rouge_score['rouge1'][2])\n",
        "        ROUGE2_Percision_Scores.append(rouge_score['rouge2'][0])\n",
        "        ROUGE2_Recall_Scores.append(rouge_score['rouge2'][1])\n",
        "        ROUGE2_F1_Scores.append(rouge_score['rouge2'][2])\n",
        "        ROUGEL_Percision_Scores.append(rouge_score['rougeL'][0])\n",
        "        ROUGEL_Recall_Scores.append(rouge_score['rougeL'][1])\n",
        "        ROUGEL_F1_Scores.append(rouge_score['rougeL'][2])\n",
        "\n",
        "    return {\n",
        "        \"rouge1\": [np.mean(ROUGE1_Percision_Scores), np.mean(ROUGE1_Recall_Scores), np.mean(ROUGE1_F1_Scores)],\n",
        "        \"rouge2\": [np.mean(ROUGE2_Percision_Scores), np.mean(ROUGE2_Recall_Scores), np.mean(ROUGE2_F1_Scores)],\n",
        "        \"rougeL\": [np.mean(ROUGEL_Percision_Scores), np.mean(ROUGEL_Recall_Scores), np.mean(ROUGEL_F1_Scores)]\n",
        "    }\n",
        "\n",
        "# Calculate BLEU and ROUGE scores\n",
        "bleu_score = CalculateAvgBLEUScore(translated_arabic_sentences, dataset['ar'], tokenizer)\n",
        "rouge_score = CalculateAvgROUGEScore(translated_arabic_sentences, dataset['ar'])\n",
        "\n",
        "# Print the results\n",
        "print(\"Average BLEU Score:\", bleu_score)\n",
        "print(\"Average ROUGE Scores:\", rouge_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr8ezhGLW-kw",
        "outputId": "0e9a56d9-9f93-446a-c1c8-b03d816f3322"
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 1.4916681462400622e-154\n",
            "Average ROUGE Scores: {'rouge1': [0.0, 0.0, 0.0], 'rouge2': [0.0, 0.0, 0.0], 'rougeL': [0.0, 0.0, 0.0]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Create a DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Reference Arabic': dataset['ar'],              # Original Arabic sentences\n",
        "    'Translated Arabic': translated_arabic_sentences  # Model's translated sentences\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "results_df.to_csv('translations_comparison.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "# Provide feedback that the file is saved\n",
        "print(\"CSV file 'translations_comparison.csv' created successfully!\")\n",
        "\n",
        "# Download the CSV file\n",
        "files.download('translations_comparison.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "0yaz3RhUiTon",
        "outputId": "761b8c66-46fc-4a6d-f3f3-a132da9ab187"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'translations_comparison.csv' created successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_034b54d1-790f-402b-97a2-27c372969498\", \"translations_comparison.csv\", 14972)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}